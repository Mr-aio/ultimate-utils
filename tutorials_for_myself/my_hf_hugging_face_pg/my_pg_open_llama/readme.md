# Open LLaMA

Decide not to use it since they trained with jax and their tokenizer bug seems annoying.
I think using falcon is fine because it's higher in the hugging face leader board + its in HF.
Maybe if they show me their pre-training code in Jax I would use it for mey side project/verify my interview practicing 
for LLMs. 
They used Jax it seems because they trained on Google TPUs. 

ref:
    - https://github.com/openlm-research/open_llama